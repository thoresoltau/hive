# Analyse: OpenAI Prompting Guides vs. Hive Agent Prompts

> Erstellt: 2025-11-28 (aktualisiert mit GPT-5.1 Guides)
> Quellen: 
> - GPT-5.1 Prompting Guide
> - GPT-5-Codex Prompting Guide
> - Build a Coding Agent with GPT-5.1
> - Codex Code Review SDK Guide

---

## Executive Summary

Die aktuellen OpenAI GPT-5.1 Prompting Guides enthalten **kritische Best Practices**, die in unseren Agent-Prompts fehlen. Besonders wichtig: **"Less is more"** - GPT-5-Codex ist fÃ¼r Coding optimiert, zu viel Prompting kann die QualitÃ¤t verschlechtern.

**PrioritÃ¤t der Ã„nderungen:**
1. ğŸ”´ **Kritisch**: Solution Persistence (`<solution_persistence>` Pattern)
2. ğŸ”´ **Kritisch**: Plan Tool Integration (2-5 Milestones)
3. ğŸŸ¡ **Wichtig**: Git-Awareness (nie fremde Ã„nderungen reverten)
4. ğŸŸ¡ **Wichtig**: Code Review Fokus (Bugs > Summaries)
5. ğŸŸ¢ **Nice-to-Have**: Minimale Prompts fÃ¼r Codex
6. ğŸŸ¢ **Nice-to-Have**: Context7 MCP Integration fÃ¼r Docs

---

## 1. Solution Persistence (GPT-5.1) ğŸ”´ KRITISCH

### Was OpenAI empfiehlt (GPT-5.1 Prompting Guide):

```xml
<solution_persistence>
- Treat yourself as an autonomous senior pair-programmer: once the user gives 
  a direction, proactively gather context, plan, implement, test, and refine 
  without waiting for additional prompts at each step.
- Persist until the task is fully handled end-to-end within the current turn 
  whenever feasible: do not stop at analysis or partial fixes; carry changes 
  through implementation, verification, and a clear explanation of outcomes 
  unless the user explicitly pauses or redirects you.
- Be extremely biased for action. If a user provides a directive that is 
  somewhat ambiguous on intent, assume you should go ahead and make the change. 
  If the user asks a question like "should we do x?" and your answer is "yes", 
  you should also go ahead and perform the action. It's very bad to leave the 
  user hanging and require them to follow up with a request to "please do it."
</solution_persistence>
```

### Was wir haben:

âŒ **Keine** Persistence-Anweisungen  
âŒ **Keine** expliziten "Don't Guess" Anweisungen  
âŒ **Keine** Planning-Aufforderungen  

### Empfehlung:

**Jeden Agent-Prompt erweitern mit:**
```yaml
## Arbeitsweise
- Arbeite autonom bis das Problem vollstÃ¤ndig gelÃ¶st ist
- Nutze IMMER Tools um Informationen zu verifizieren - rate NIEMALS
- Plane jeden Schritt BEVOR du ein Tool aufrufst
- Reflektiere nach jedem Tool-Aufruf Ã¼ber das Ergebnis
```

---

## 2. Strukturierter Workflow (SWE-bench Pattern)

### Was OpenAI empfiehlt:

Der SWE-bench Prompt definiert einen **8-Schritte Workflow**:

1. **Understand the problem deeply** - Carefully read and think critically
2. **Investigate the codebase** - Explore files, search functions, gather context
3. **Develop a detailed plan** - Break down into small, incremental steps
4. **Implement incrementally** - Make small, testable changes
5. **Debug as needed** - Determine root cause, not symptoms
6. **Test frequently** - Run tests after each change
7. **Final verification** - Review solution for correctness
8. **Final reflection** - Think about edge cases, write additional tests

### Was wir haben:

Unsere Prompts haben:
- âœ… Aufgaben-Liste
- âœ… Tool-Beschreibungen
- âŒ **Keinen** strukturierten Workflow
- âŒ **Keine** expliziten Debugging-Anweisungen
- âŒ **Keine** Test-Betonung

### Empfehlung:

**Workflow-Sektion zu allen Dev-Agents hinzufÃ¼gen:**
```yaml
## Workflow

### 1. Problem verstehen
- Lies das Ticket und alle Acceptance Criteria sorgfÃ¤ltig
- Stelle RÃ¼ckfragen wenn etwas unklar ist

### 2. Codebase untersuchen
- Nutze list_directory und find_files um relevante Dateien zu finden
- Lies existierenden Code um Patterns zu verstehen
- Identifiziere die Root Cause bei Bugs

### 3. Plan erstellen
- Brich die Aufgabe in kleine, testbare Schritte
- Dokumentiere deinen Plan bevor du implementierst

### 4. Inkrementell implementieren
- Mache kleine Ã„nderungen
- Committe nach jedem logischen Schritt

### 5. Testen
âš ï¸ KRITISCH: Unzureichendes Testen ist der hÃ¤ufigste Fehlergrund!
- FÃ¼hre Tests nach JEDER Ã„nderung aus
- Teste Edge Cases
- Schreibe zusÃ¤tzliche Tests wenn nÃ¶tig

### 6. Verifizieren
- PrÃ¼fe ob alle Acceptance Criteria erfÃ¼llt sind
- FÃ¼hre einen finalen Test-Durchlauf durch
```

---

## 3. Code Review Prompt

### Was OpenAI empfiehlt:

```
You are acting as a reviewer for a proposed code change made by another engineer. 
Focus on issues that impact correctness, performance, security, maintainability, 
or developer experience. Flag only actionable issues introduced by the pull request. 
When you flag an issue, provide a short, direct explanation and cite the affected 
file and line range. Prioritize severe issues and avoid nit-level comments unless 
they block understanding of the diff. After listing findings, produce an overall 
correctness verdict ("patch is correct" or "patch is incorrect") with a concise 
justification and a confidence score between 0 and 1.
```

### Was wir haben:

Unser Architect-Prompt fÃ¼r Code Review:
- âœ… PrÃ¼ft Code-QualitÃ¤t
- âœ… PrÃ¼ft Architektur-KonformitÃ¤t
- âŒ **Keine** PrioritÃ¤tsstufen fÃ¼r Issues
- âŒ **Kein** Confidence Score
- âŒ **Keine** Fokussierung auf "actionable issues"

### Empfehlung:

**Code-Review Prompt verbessern:**
```yaml
## Code Review
FÃ¼hre ein Review durch und fokussiere auf:
1. **Korrektheit** - Funktioniert der Code wie erwartet?
2. **Sicherheit** - Gibt es Security-Vulnerabilities?
3. **Performance** - Gibt es Performance-Probleme?
4. **Wartbarkeit** - Ist der Code lesbar und wartbar?

### Regeln:
- Flagge NUR actionable Issues, die durch diese Ã„nderung eingefÃ¼hrt wurden
- Priorisiere schwerwiegende Issues Ã¼ber Nit-Picking
- Gib fÃ¼r jedes Issue: Datei, Zeile, Severity (error/warning/info), ErklÃ¤rung
- Gib am Ende ein Verdict: "approved" oder "changes_requested"
- Gib einen Confidence Score (0-1) fÃ¼r dein Review
```

---

## 4. Prompt-Struktur

### Was OpenAI empfiehlt:

```markdown
# Role and Objective
# Instructions
## Sub-categories for detailed instructions
# Reasoning Steps
# Output Format
# Examples
## Example 1
# Context
# Final instructions and prompt to think step by step
```

### Was wir haben:

```yaml
Du bist ein erfahrener [Rolle].

## Deine Aufgaben
- ...

## VerfÃ¼gbare Tools
- ...
```

### Empfehlung:

**Prompts umstrukturieren:**
```yaml
# Rolle und Ziel
Du bist der [Rolle] im Hive Agent Swarm Team.
Dein Ziel: [klares, messbares Ziel]

# Anweisungen
## Kernaufgaben
- ...

## Arbeitsweise
- Arbeite autonom bis das Problem gelÃ¶st ist
- Nutze Tools - rate NIEMALS
- Plane vor jedem Tool-Aufruf

# Workflow
1. ...
2. ...

# VerfÃ¼gbare Tools
## File-Tools
- read_file: [Beschreibung + wann nutzen]
...

# Output-Format
[Erwartetes Format der Antworten]

# Beispiele
## Beispiel: Ticket analysieren
[Konkretes Beispiel]

# Kontext
[Projekt-spezifischer Kontext]

# AbschlieÃŸende Anweisung
Denke Schritt fÃ¼r Schritt. Verifiziere jeden Schritt bevor du fortfÃ¤hrst.
```

---

## 5. Delimiters

### Was OpenAI empfiehlt:

- **Markdown** (empfohlen): `#`, `##`, ``` fÃ¼r Code
- **XML**: Gut fÃ¼r strukturierte Daten
- **JSON**: Vermeiden bei Long Context

### Was wir haben:

âœ… Wir nutzen bereits Markdown - das ist gut!

### Empfehlung:

Keine Ã„nderung nÃ¶tig, aber konsistenter sein bei der Verwendung.

---

## 6. Tool-Beschreibungen

### Was OpenAI empfiehlt:

> "Developers should name tools clearly to indicate their purpose and add a clear, 
> detailed description in the 'description' field of the tool."

> "If your tool is particularly complicated and you'd like to provide examples of 
> tool usage, we recommend that you create an # Examples section in your system 
> prompt and place the examples there."

### Was wir haben:

Unsere Tool-Beschreibungen sind kurz:
```python
description = "LÃ¶scht eine Datei. Kann keine Verzeichnisse lÃ¶schen."
```

### Empfehlung:

**Tool-Beschreibungen erweitern:**
```python
description = """LÃ¶scht eine einzelne Datei permanent.

Wann nutzen:
- Zum Entfernen von temporÃ¤ren oder generierten Dateien
- Beim AufrÃ¤umen nach Refactoring

EinschrÃ¤nkungen:
- Kann KEINE Verzeichnisse lÃ¶schen (nutze dafÃ¼r delete_directory)
- GelÃ¶schte Dateien kÃ¶nnen nicht wiederhergestellt werden
"""
```

---

## Implementierungsplan

### Phase 1: Kritische Ã„nderungen (sofort)

1. **Persistence/Planning/Don't-Guess Reminders** zu allen Prompts hinzufÃ¼gen
2. **Workflow-Sektion** zu Backend/Frontend Dev hinzufÃ¼gen
3. **Test-Betonung** verstÃ¤rken

### Phase 2: Wichtige Verbesserungen (nÃ¤chste Iteration)

4. **Code-Review Prompt** mit PrioritÃ¤ten und Confidence Score
5. **Prompt-Struktur** vereinheitlichen nach OpenAI-Template
6. **Tool-Beschreibungen** erweitern

### Phase 3: Nice-to-Have (spÃ¤ter)

7. **Beispiele** zu jedem Prompt hinzufÃ¼gen
8. **Chain-of-Thought** explizit anfordern
9. **Structured Output** fÃ¼r Reviews

---

## Konkrete Ã„nderungen fÃ¼r agents.yaml

### Neue gemeinsame Basis-Sektion:

```yaml
# FÃ¼ge zu JEDEM Agent hinzu:

## Arbeitsweise (WICHTIG)
- Du bist ein autonomer Agent - arbeite bis das Problem VOLLSTÃ„NDIG gelÃ¶st ist
- Beende NIEMALS ohne LÃ¶sung, auÃŸer du brauchst explizit Input
- Nutze IMMER deine Tools um Informationen zu verifizieren
- Rate oder erfinde NIEMALS Antworten
- Plane JEDEN Schritt bevor du ein Tool aufrufst
- Reflektiere NACH jedem Tool-Aufruf Ã¼ber das Ergebnis
```

### Backend Dev - Neue Test-Sektion:

```yaml
## Testing (KRITISCH)
âš ï¸ Unzureichendes Testen ist der #1 Grund fÃ¼r Fehler!

1. FÃ¼hre Tests NACH JEDER Code-Ã„nderung aus
2. Teste alle Edge Cases
3. Schreibe zusÃ¤tzliche Tests wenn nÃ¶tig
4. Beende erst wenn ALLE Tests grÃ¼n sind
```

---

## Zusammenfassung

| Bereich | Aktuell | Empfohlen | PrioritÃ¤t |
|---------|---------|-----------|-----------|
| Persistence Reminders | âŒ | âœ… | ğŸ”´ Kritisch |
| Don't Guess Anweisung | âŒ | âœ… | ğŸ”´ Kritisch |
| Planning Anweisung | âŒ | âœ… | ğŸ”´ Kritisch |
| Strukturierter Workflow | âŒ | âœ… | ğŸŸ¡ Wichtig |
| Test-Betonung | ğŸŸ¡ | âœ… | ğŸŸ¡ Wichtig |
| Code Review Prompt | ğŸŸ¡ | âœ… | ğŸŸ¡ Wichtig |
| Prompt-Struktur | ğŸŸ¡ | âœ… | ğŸŸ¢ Nice-to-Have |
| Tool-Beschreibungen | ğŸŸ¡ | âœ… | ğŸŸ¢ Nice-to-Have |
| Beispiele | âŒ | âœ… | ğŸŸ¢ Nice-to-Have |

**GeschÃ¤tzter Impact der Ã„nderungen: +15-25% bessere Task-Completion**  
(basierend auf OpenAIs eigenen SWE-bench Ergebnissen: +20% durch Reminders)
